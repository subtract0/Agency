"""
Meta-testing and self-reference paradox scenarios for the CodeHealer framework.
Addresses the philosophical challenges of self-healing quality assurance systems.

This file explores and tests:
1. The paradox of quality tools assessing their own quality
2. Infinite recursion prevention in self-testing scenarios
3. Meta-cognitive aspects of automated quality assessment
4. Bootstrap scenarios for self-improving test frameworks
5. Convergence and stability in recursive quality assessment

Generated by TestGeneratorAgent for philosophical completeness testing.
"""

import os
import json
import ast
import tempfile
import time
import threading
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, List, Any, Set

import pytest

from auditor_agent.auditor_agent import create_auditor_agent, AnalyzeCodebase
from test_generator_agent.test_generator_agent import create_test_generator_agent, GenerateTests
from shared.agent_context import create_agent_context


class TestSelfReferenceParadoxes:
    """Test scenarios involving self-reference paradoxes in quality assessment."""

    def test_auditor_auditing_auditor_logic(self):
        """Test the paradox of an auditor auditing audit logic."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a simplified auditor that can audit audit logic
            meta_auditor = Path(temp_dir) / "meta_auditor.py"
            meta_auditor.write_text('''
class MetaAuditor:
    """Auditor that can audit audit logic."""

    def __init__(self):
        self.audits_performed = 0

    def audit_code(self, code_to_audit):
        """Audit given code for quality."""
        self.audits_performed += 1

        if not code_to_audit:
            return {"quality_score": 0.0, "issues": ["Empty code"]}

        # Simple quality heuristics
        issues = []
        quality_score = 1.0

        if "def audit_code" in code_to_audit:
            # This is audit logic - apply meta-audit rules
            if "audits_performed" not in code_to_audit:
                issues.append("Audit logic should track audit count")
                quality_score -= 0.2

            if "quality_score" not in code_to_audit:
                issues.append("Audit logic should calculate quality scores")
                quality_score -= 0.3

            if "meta-audit" in code_to_audit.lower():
                # Recursive audit detection
                issues.append("Potential infinite recursion in meta-audit")
                quality_score -= 0.5

        if len(code_to_audit.split("\\n")) > 100:
            issues.append("Code is too long")
            quality_score -= 0.1

        return {
            "quality_score": max(0.0, quality_score),
            "issues": issues,
            "auditor_id": id(self)
        }

    def audit_auditor(self, other_auditor_code):
        """Audit another auditor's code - meta-auditing."""
        # This creates a potential paradox
        return self.audit_code(other_auditor_code)

    def audit_self(self):
        """The ultimate paradox - audit own code."""
        # This should be handled carefully to avoid infinite recursion
        import inspect
        own_source = inspect.getsource(MetaAuditor)
        return self.audit_code(own_source)
''')

            # Test that we can audit the meta-auditor without infinite recursion
            tool = AnalyzeCodebase(target_path=temp_dir)

            start_time = time.time()
            result = tool.run()
            end_time = time.time()

            # Should complete without hanging
            assert (end_time - start_time) < 10, "Meta-audit should not cause infinite recursion"

            result_data = json.loads(result)
            assert "qt_score" in result_data

            # Should detect the audit methods
            codebase_analysis = result_data["codebase_analysis"]
            assert codebase_analysis["total_behaviors"] >= 3  # audit_code, audit_auditor, audit_self

    def test_test_generator_generating_tests_for_test_generators(self):
        """Test the paradox of test generators creating tests for test generation."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a test generator that can generate tests for test generation logic
            meta_generator = Path(temp_dir) / "meta_test_generator.py"
            meta_generator.write_text('''
import ast

class MetaTestGenerator:
    """Test generator that can generate tests for test generators."""

    def __init__(self):
        self.tests_generated = []

    def generate_test_for_function(self, function_name, function_code):
        """Generate test for a given function."""
        if "generate_test" in function_name:
            # Meta-test generation - be careful of infinite loops
            return self._generate_meta_test(function_name, function_code)
        else:
            return self._generate_regular_test(function_name, function_code)

    def _generate_regular_test(self, func_name, func_code):
        """Generate regular test."""
        test_code = f'''
def test_{func_name}_basic():
    """Test {func_name} functionality."""
    # Test implementation here
    assert True  # Placeholder
'''
        self.tests_generated.append(test_code)
        return test_code

    def _generate_meta_test(self, func_name, func_code):
        """Generate test for test generation logic."""
        # This is where the paradox lives - testing test generation

        if "generate_meta_test" in func_code:
            # We're generating a test for meta-test generation
            # Limit recursion depth to prevent infinite loops
            test_code = f'''
def test_{func_name}_meta():
    """Test meta-test generation logic."""
    generator = MetaTestGenerator()

    # Test that meta-test generation doesn't recurse infinitely
    result = generator.{func_name}("sample_func", "def sample_func(): pass")
    assert "def test_" in result
    assert len(result) > 0

    # Test recursion prevention
    recursive_code = "def generate_meta_test(): return generate_meta_test()"
    result2 = generator.{func_name}("generate_meta_test", recursive_code)
    assert result2 is not None  # Should handle gracefully
'''
        else:
            test_code = f'''
def test_{func_name}_generation():
    """Test test generation functionality."""
    generator = MetaTestGenerator()

    result = generator.{func_name}("example_func", "def example_func(): return True")
    assert "def test_" in result
    assert "example_func" in result
'''

        self.tests_generated.append(test_code)
        return test_code

    def analyze_and_generate_tests_for_self(self):
        """Analyze this generator and generate tests for it."""
        import inspect

        # Get our own source code
        own_source = inspect.getsource(MetaTestGenerator)

        # Parse to find methods
        tree = ast.parse(own_source)
        methods = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                methods.append({
                    "name": node.name,
                    "code": own_source  # Simplified
                })

        # Generate tests for our own methods
        self_tests = []
        for method in methods:
            if not method["name"].startswith("_"):  # Public methods only
                test = self.generate_test_for_function(method["name"], method["code"])
                self_tests.append(test)

        return self_tests
''')

            # Try to generate tests for this meta-generator
            audit_report = {
                "violations": [
                    {"property": "N", "severity": "critical", "description": "Meta-testing needed"}
                ]
            }

            tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(meta_generator)
            )

            start_time = time.time()
            result = tool.run()
            end_time = time.time()

            # Should complete without infinite recursion
            assert (end_time - start_time) < 15, "Meta-test generation should not hang"

            result_data = json.loads(result)
            assert result_data["status"] == "success"

            # Should generate tests for the meta-generator methods
            assert result_data["tests_generated"] > 0

    def test_circular_dependency_detection(self):
        """Test detection and handling of circular dependencies in quality assessment."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create two files that depend on each other for quality assessment
            assessor_a = Path(temp_dir) / "quality_assessor_a.py"
            assessor_a.write_text('''
from quality_assessor_b import QualityAssessorB

class QualityAssessorA:
    """Quality assessor A that depends on B for validation."""

    def __init__(self):
        self.assessor_b = None

    def assess_quality(self, code):
        """Assess quality using B as reference."""
        if self.assessor_b is None:
            self.assessor_b = QualityAssessorB()

        # Ask B to validate our assessment
        our_score = self._calculate_score(code)
        b_validation = self.assessor_b.validate_assessment(our_score, code)

        return {
            "score": our_score,
            "validated": b_validation,
            "assessor": "A"
        }

    def _calculate_score(self, code):
        """Calculate quality score."""
        if not code:
            return 0.0

        lines = code.split("\\n")
        return min(1.0, len(lines) / 100)

    def validate_assessment(self, score, code):
        """Validate another assessor's assessment."""
        our_score = self._calculate_score(code)
        return abs(score - our_score) < 0.1
''')

            assessor_b = Path(temp_dir) / "quality_assessor_b.py"
            assessor_b.write_text('''
# Note: This would normally import QualityAssessorA, creating circular dependency
# from quality_assessor_a import QualityAssessorA

class QualityAssessorB:
    """Quality assessor B that can validate A's assessments."""

    def __init__(self):
        self.assessor_a = None

    def assess_quality(self, code):
        """Assess quality independently."""
        score = self._calculate_score(code)

        # We could ask A to validate, but that creates circularity
        # if self.assessor_a is None:
        #     from quality_assessor_a import QualityAssessorA
        #     self.assessor_a = QualityAssessorA()
        # a_validation = self.assessor_a.validate_assessment(score, code)

        return {
            "score": score,
            "validated": True,  # Skip circular validation
            "assessor": "B"
        }

    def _calculate_score(self, code):
        """Calculate quality score using different metrics."""
        if not code:
            return 0.0

        # Different scoring algorithm than A
        function_count = code.count("def ")
        class_count = code.count("class ")
        return min(1.0, (function_count + class_count) / 10)

    def validate_assessment(self, score, code):
        """Validate another assessor's assessment."""
        our_score = self._calculate_score(code)
        return abs(score - our_score) < 0.2  # More lenient than A
''')

            # Audit this circular system
            tool = AnalyzeCodebase(target_path=temp_dir)

            start_time = time.time()
            result = tool.run()
            end_time = time.time()

            # Should handle circular dependencies gracefully
            assert (end_time - start_time) < 10, "Should handle circular dependencies"

            result_data = json.loads(result)
            assert "qt_score" in result_data

            # Should detect both classes and their methods
            codebase_analysis = result_data["codebase_analysis"]
            assert codebase_analysis["total_behaviors"] >= 4  # Multiple methods across files

    def test_bootstrap_paradox_in_test_frameworks(self):
        """Test the bootstrap paradox - how do you test a test framework with itself?"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create a minimal test framework that must test itself
            test_framework = Path(temp_dir) / "bootstrap_framework.py"
            test_framework.write_text('''
class BootstrapTestFramework:
    """Minimal test framework that must bootstrap its own testing."""

    def __init__(self):
        self.test_results = []
        self.assertions_made = 0

    def assert_equal(self, expected, actual):
        """Basic assertion method."""
        self.assertions_made += 1
        if expected == actual:
            self.test_results.append({"status": "pass", "assertion": f"{expected} == {actual}"})
            return True
        else:
            self.test_results.append({"status": "fail", "assertion": f"{expected} != {actual}"})
            return False

    def run_test(self, test_function):
        """Run a test function."""
        try:
            test_function(self)
            return {"status": "success", "assertions": self.assertions_made}
        except Exception as e:
            return {"status": "error", "error": str(e)}

    def bootstrap_self_test(self):
        """Test this framework using itself - the bootstrap paradox."""

        def test_assert_equal(framework):
            """Test the assert_equal method."""
            # Use the framework to test itself
            framework.assert_equal(1, 1)  # Should pass
            framework.assert_equal(1, 2)  # Should fail

        def test_run_test(framework):
            """Test the run_test method."""
            def dummy_test(fw):
                fw.assert_equal("hello", "hello")

            result = framework.run_test(dummy_test)
            framework.assert_equal("success", result["status"])

        # Run tests using ourselves
        result1 = self.run_test(test_assert_equal)
        result2 = self.run_test(test_run_test)

        return {
            "bootstrap_results": [result1, result2],
            "total_assertions": self.assertions_made,
            "test_results": self.test_results
        }

    def validate_bootstrap_results(self):
        """Validate that bootstrap testing worked correctly."""
        bootstrap_results = self.bootstrap_self_test()

        # Meta-validation: check that our self-test actually tested us
        total_assertions = bootstrap_results["total_assertions"]
        test_results = bootstrap_results["test_results"]

        # Should have made assertions
        if total_assertions == 0:
            return {"valid": False, "reason": "No assertions made during bootstrap"}

        # Should have both passing and failing assertions (testing both paths)
        passes = sum(1 for r in test_results if r["status"] == "pass")
        fails = sum(1 for r in test_results if r["status"] == "fail")

        if passes == 0:
            return {"valid": False, "reason": "No passing assertions - framework might be broken"}

        if fails == 0:
            return {"valid": False, "reason": "No failing assertions - might not be testing failure cases"}

        return {
            "valid": True,
            "passes": passes,
            "fails": fails,
            "total": total_assertions
        }
''')

            # Test the bootstrap framework
            audit_report = {
                "violations": [
                    {"property": "Y", "severity": "critical", "description": "Bootstrap validation needed"}
                ]
            }

            # First, audit the framework
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            # Should detect the bootstrap methods
            assert audit_data["codebase_analysis"]["total_behaviors"] >= 4

            # Then generate tests for the bootstrap framework
            generate_tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(test_framework)
            )

            generation_result = generate_tool.run()
            generation_data = json.loads(generation_result)

            assert generation_data["status"] == "success"
            assert generation_data["tests_generated"] > 0

            # The generated tests should test the test framework
            test_file_path = generation_data["test_file"]
            with open(test_file_path, 'r') as f:
                test_content = f.read()

            # Should include tests for the bootstrap methods
            assert "test_bootstrap" in test_content.lower() or "test_assert" in test_content.lower()


class TestMetaCognitiveQualityAssessment:
    """Test meta-cognitive aspects of automated quality assessment."""

    def test_quality_assessor_assessing_assessment_quality(self):
        """Test a quality assessor that assesses the quality of quality assessment."""
        with tempfile.TemporaryDirectory() as temp_dir:
            meta_assessor = Path(temp_dir) / "meta_quality_assessor.py"
            meta_assessor.write_text('''
class MetaQualityAssessor:
    """Assesses the quality of quality assessment processes."""

    def __init__(self):
        self.assessment_history = []

    def assess_code_quality(self, code):
        """Traditional code quality assessment."""
        metrics = {
            "lines_of_code": len(code.split("\\n")),
            "function_count": code.count("def "),
            "class_count": code.count("class "),
            "comment_ratio": code.count("#") / max(1, len(code.split("\\n")))
        }

        # Simple quality score
        score = min(1.0, (metrics["function_count"] + metrics["class_count"]) / 10)
        if metrics["comment_ratio"] > 0.1:
            score += 0.1

        assessment = {
            "code_quality_score": score,
            "metrics": metrics,
            "timestamp": "now"  # Simplified
        }

        self.assessment_history.append(assessment)
        return assessment

    def assess_assessment_quality(self, assessment_result):
        """Meta-assessment: assess the quality of a quality assessment."""
        quality_indicators = []

        # Check if assessment has required components
        if "code_quality_score" not in assessment_result:
            quality_indicators.append("Missing quality score")

        if "metrics" not in assessment_result:
            quality_indicators.append("Missing metrics")

        # Check score validity
        score = assessment_result.get("code_quality_score", 0)
        if not isinstance(score, (int, float)) or not (0 <= score <= 1):
            quality_indicators.append("Invalid quality score range")

        # Check metrics validity
        metrics = assessment_result.get("metrics", {})
        if isinstance(metrics, dict):
            if "lines_of_code" in metrics and metrics["lines_of_code"] < 0:
                quality_indicators.append("Negative lines of code")

            if "function_count" in metrics and metrics["function_count"] < 0:
                quality_indicators.append("Negative function count")

        # Meta-quality score
        meta_score = 1.0 - (len(quality_indicators) * 0.2)

        return {
            "assessment_quality_score": max(0.0, meta_score),
            "quality_indicators": quality_indicators,
            "assessed_assessment": assessment_result
        }

    def assess_meta_assessment_quality(self, meta_assessment_result):
        """Meta-meta-assessment: assess the quality of assessment quality assessment."""
        # This is getting very recursive - be careful!

        if "assessment_quality_score" not in meta_assessment_result:
            return {"error": "Invalid meta-assessment structure"}

        meta_meta_score = 1.0
        issues = []

        # Check if meta-assessment has proper structure
        if "quality_indicators" not in meta_assessment_result:
            issues.append("Meta-assessment missing quality indicators")
            meta_meta_score -= 0.3

        if "assessed_assessment" not in meta_assessment_result:
            issues.append("Meta-assessment missing reference to original assessment")
            meta_meta_score -= 0.3

        # Check logical consistency
        original_score = meta_assessment_result.get("assessed_assessment", {}).get("code_quality_score", 0)
        meta_score = meta_assessment_result.get("assessment_quality_score", 0)

        # If original assessment had high quality but meta-assessment rated it low,
        # that might indicate issues with meta-assessment logic
        if original_score > 0.8 and meta_score < 0.3:
            issues.append("Possible inconsistency in meta-assessment logic")
            meta_meta_score -= 0.2

        return {
            "meta_meta_score": max(0.0, meta_meta_score),
            "meta_assessment_issues": issues,
            "recursion_depth": 3  # Track how deep we've gone
        }

    def full_recursive_assessment(self, code, max_depth=3):
        """Perform recursive quality assessment up to specified depth."""
        results = []

        # Level 1: Assess code
        code_assessment = self.assess_code_quality(code)
        results.append({"level": 1, "type": "code_assessment", "result": code_assessment})

        # Level 2: Assess the assessment
        if max_depth >= 2:
            meta_assessment = self.assess_assessment_quality(code_assessment)
            results.append({"level": 2, "type": "meta_assessment", "result": meta_assessment})

            # Level 3: Assess the meta-assessment
            if max_depth >= 3:
                meta_meta_assessment = self.assess_meta_assessment_quality(meta_assessment)
                results.append({"level": 3, "type": "meta_meta_assessment", "result": meta_meta_assessment})

                # We could go deeper, but that's probably not useful and risks infinite recursion

        return {
            "recursive_assessment_results": results,
            "max_depth_reached": len(results),
            "convergence_indicator": self._check_convergence(results)
        }

    def _check_convergence(self, assessment_results):
        """Check if recursive assessments are converging to stable values."""
        if len(assessment_results) < 2:
            return "insufficient_data"

        scores = []
        for result in assessment_results:
            if result["type"] == "code_assessment":
                scores.append(result["result"]["code_quality_score"])
            elif result["type"] == "meta_assessment":
                scores.append(result["result"]["assessment_quality_score"])
            elif result["type"] == "meta_meta_assessment":
                scores.append(result["result"]["meta_meta_score"])

        if len(scores) >= 2:
            variance = sum((s - sum(scores)/len(scores))**2 for s in scores) / len(scores)
            if variance < 0.01:
                return "converging"
            else:
                return "diverging"

        return "unclear"
''')

            # Audit this meta-cognitive assessor
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            # Should detect multiple assessment methods
            assert audit_data["codebase_analysis"]["total_behaviors"] >= 5

            # Generate tests for meta-cognitive assessment
            audit_report = {
                "violations": [
                    {"property": "Y", "severity": "critical", "description": "Meta-cognitive testing needed"}
                ]
            }

            generate_tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(meta_assessor)
            )

            generation_result = generate_tool.run()
            generation_data = json.loads(generation_result)

            assert generation_data["status"] == "success"
            assert generation_data["tests_generated"] > 0

    def test_convergence_detection_in_recursive_assessment(self):
        """Test detection of convergence in recursive quality assessment."""
        with tempfile.TemporaryDirectory() as temp_dir:
            convergence_tester = Path(temp_dir) / "convergence_tester.py"
            convergence_tester.write_text('''
class ConvergenceDetector:
    """Detects convergence in recursive assessment processes."""

    def __init__(self, convergence_threshold=0.01):
        self.threshold = convergence_threshold
        self.history = []

    def iterative_assessment(self, initial_value, max_iterations=10):
        """Perform iterative assessment until convergence."""
        current_value = initial_value
        values = [current_value]

        for i in range(max_iterations):
            # Simulate assessment function that converges
            next_value = self.assessment_function(current_value)
            values.append(next_value)

            # Check for convergence
            if abs(next_value - current_value) < self.threshold:
                return {
                    "converged": True,
                    "iterations": i + 1,
                    "final_value": next_value,
                    "values": values
                }

            current_value = next_value

        return {
            "converged": False,
            "iterations": max_iterations,
            "final_value": current_value,
            "values": values
        }

    def assessment_function(self, x):
        """Assessment function that should converge."""
        # Simulate a quality assessment that improves iteratively
        # but with diminishing returns (convergent)
        return x + (1 - x) * 0.3

    def divergent_assessment_function(self, x):
        """Assessment function that diverges."""
        # This will not converge - for testing divergence detection
        return x * 1.1 if x < 1 else x + 0.1

    def oscillating_assessment_function(self, x):
        """Assessment function that oscillates."""
        # This creates oscillation rather than convergence
        return 1 - x

    def test_convergence_patterns(self):
        """Test different convergence patterns."""
        patterns = {}

        # Test convergent function
        result1 = self.iterative_assessment(0.1)
        patterns["convergent"] = result1

        # Test divergent function
        self.assessment_function = self.divergent_assessment_function
        result2 = self.iterative_assessment(0.1)
        patterns["divergent"] = result2

        # Test oscillating function
        self.assessment_function = self.oscillating_assessment_function
        result3 = self.iterative_assessment(0.3)
        patterns["oscillating"] = result3

        return patterns

    def analyze_stability(self, assessment_sequence):
        """Analyze stability of assessment sequence."""
        if len(assessment_sequence) < 3:
            return {"stable": "insufficient_data"}

        # Calculate variance of last few values
        recent_values = assessment_sequence[-5:]  # Last 5 values
        mean_recent = sum(recent_values) / len(recent_values)
        variance = sum((v - mean_recent)**2 for v in recent_values) / len(recent_values)

        # Calculate trend
        if len(assessment_sequence) >= 2:
            slope = assessment_sequence[-1] - assessment_sequence[-2]
        else:
            slope = 0

        stability_analysis = {
            "variance": variance,
            "trend_slope": slope,
            "mean_recent": mean_recent,
            "stable": variance < self.threshold and abs(slope) < self.threshold
        }

        return stability_analysis
''')

            # Test the convergence detector
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            # Should detect the convergence analysis methods
            assert audit_data["codebase_analysis"]["total_behaviors"] >= 5

            # This is itself a meta-test of convergence in quality assessment
            # The fact that we can analyze convergence demonstrates meta-cognitive capability


class TestInfiniteRecursionPrevention:
    """Test prevention of infinite recursion in self-referential scenarios."""

    def test_recursion_depth_limiting(self):
        """Test that recursion depth is properly limited."""
        with tempfile.TemporaryDirectory() as temp_dir:
            recursive_analyzer = Path(temp_dir) / "recursive_analyzer.py"
            recursive_analyzer.write_text('''
import sys

class RecursionSafeAnalyzer:
    """Analyzer that prevents infinite recursion."""

    def __init__(self, max_depth=5):
        self.max_depth = max_depth
        self.current_depth = 0
        self.analysis_stack = []

    def analyze(self, target, analysis_id=None):
        """Analyze target with recursion protection."""
        if analysis_id is None:
            analysis_id = f"analysis_{len(self.analysis_stack)}"

        # Check for circular references
        if analysis_id in self.analysis_stack:
            return {
                "error": "Circular reference detected",
                "stack": self.analysis_stack.copy(),
                "target": str(target)[:50]  # Truncate for safety
            }

        # Check depth limit
        if self.current_depth >= self.max_depth:
            return {
                "error": "Maximum recursion depth reached",
                "depth": self.current_depth,
                "max_depth": self.max_depth
            }

        # Perform analysis with protection
        self.current_depth += 1
        self.analysis_stack.append(analysis_id)

        try:
            result = self._perform_analysis(target)
            result["analysis_depth"] = self.current_depth
            result["analysis_id"] = analysis_id
            return result
        finally:
            self.current_depth -= 1
            self.analysis_stack.pop()

    def _perform_analysis(self, target):
        """Internal analysis logic."""
        if isinstance(target, str) and "analyze" in target:
            # This is analysis code - could trigger recursive analysis
            # Simulate meta-analysis
            return {
                "type": "meta_analysis",
                "complexity": len(target),
                "contains_analysis_logic": True,
                "risk_level": "high"
            }
        else:
            return {
                "type": "regular_analysis",
                "complexity": len(str(target)),
                "contains_analysis_logic": False,
                "risk_level": "low"
            }

    def analyze_analyzer(self, analyzer_code):
        """Analyze analyzer code - potential for infinite recursion."""
        return self.analyze(analyzer_code, "meta_analysis")

    def analyze_self(self):
        """Analyze this analyzer - ultimate recursion test."""
        import inspect
        own_source = inspect.getsource(RecursionSafeAnalyzer)
        return self.analyze(own_source, "self_analysis")

    def stress_test_recursion_protection(self):
        """Stress test the recursion protection mechanisms."""
        results = []

        # Test 1: Direct self-analysis
        result1 = self.analyze_self()
        results.append({"test": "self_analysis", "result": result1})

        # Test 2: Circular analysis chain
        def create_circular_chain():
            return "analyze(analyze(analyze(target)))"

        circular_code = create_circular_chain()
        result2 = self.analyze(circular_code, "circular_test")
        results.append({"test": "circular_analysis", "result": result2})

        # Test 3: Deep recursion attempt
        deep_target = "analyze(" * 20 + "target" + ")" * 20
        result3 = self.analyze(deep_target, "deep_recursion_test")
        results.append({"test": "deep_recursion", "result": result3})

        return {
            "stress_test_results": results,
            "protection_effectiveness": all(
                "error" in r["result"] or r["result"].get("analysis_depth", 0) <= self.max_depth
                for r in results
            )
        }
''')

            # Test the recursion-safe analyzer
            audit_tool = AnalyzeCodebase(target_path=temp_dir)

            start_time = time.time()
            audit_result = audit_tool.run()
            end_time = time.time()

            # Should complete quickly without hanging
            assert (end_time - start_time) < 10, "Should prevent infinite recursion"

            audit_data = json.loads(audit_result)
            assert "qt_score" in audit_data

            # Should detect the recursion protection methods
            assert audit_data["codebase_analysis"]["total_behaviors"] >= 4

    def test_stack_overflow_prevention(self):
        """Test prevention of stack overflow in deeply recursive scenarios."""
        with tempfile.TemporaryDirectory() as temp_dir:
            stack_safe_processor = Path(temp_dir) / "stack_safe_processor.py"
            stack_safe_processor.write_text('''
import sys

class StackSafeProcessor:
    """Processor that prevents stack overflow."""

    def __init__(self):
        self.max_stack_depth = 100  # Conservative limit
        self.call_count = 0

    def process_with_stack_check(self, data, operation="process"):
        """Process data with stack overflow protection."""
        # Check current stack depth
        current_stack_depth = len(inspect.stack()) if hasattr(sys, 'stack') else 0

        # Alternative: use call count as proxy
        self.call_count += 1

        try:
            if self.call_count > self.max_stack_depth:
                return {
                    "error": "Stack depth limit reached",
                    "call_count": self.call_count,
                    "data_size": len(str(data))
                }

            # Simulate processing that might recurse
            if isinstance(data, str) and "process" in data.lower():
                # This is recursive processing request
                return self._handle_recursive_processing(data)
            else:
                return self._handle_normal_processing(data)

        finally:
            self.call_count -= 1

    def _handle_recursive_processing(self, data):
        """Handle recursive processing requests safely."""
        if self.call_count > self.max_stack_depth // 2:  # Be extra cautious
            return {
                "type": "recursive_processing",
                "result": "truncated_due_to_depth",
                "depth": self.call_count
            }

        # Simulate some recursive processing
        simplified_data = data[:100]  # Truncate to prevent exponential growth
        return {
            "type": "recursive_processing",
            "result": f"processed_{simplified_data}",
            "depth": self.call_count
        }

    def _handle_normal_processing(self, data):
        """Handle normal processing."""
        return {
            "type": "normal_processing",
            "result": f"processed_{len(str(data))}_chars",
            "depth": self.call_count
        }

    def test_deep_processing(self, depth_to_test):
        """Test processing at specified depth."""
        results = []

        # Create nested processing requests
        nested_data = "process(" * depth_to_test + "data" + ")" * depth_to_test

        result = self.process_with_stack_check(nested_data)
        results.append(result)

        return {
            "deep_processing_test": results,
            "max_depth_tested": depth_to_test,
            "protection_triggered": any("error" in r for r in results)
        }
''')

            # Audit the stack-safe processor
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            # Should complete safely
            assert "qt_score" in audit_data

            # Generate tests that verify stack safety
            audit_report = {
                "violations": [
                    {"property": "R", "severity": "critical", "description": "Stack safety testing needed"}
                ]
            }

            generate_tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(stack_safe_processor)
            )

            generation_result = generate_tool.run()
            generation_data = json.loads(generation_result)

            assert generation_data["status"] == "success"
            assert generation_data["tests_generated"] > 0

    def test_concurrent_recursion_safety(self):
        """Test recursion safety in concurrent scenarios."""
        import threading
        import queue

        with tempfile.TemporaryDirectory() as temp_dir:
            concurrent_analyzer = Path(temp_dir) / "concurrent_analyzer.py"
            concurrent_analyzer.write_text('''
import threading
import time

class ConcurrentRecursionSafeAnalyzer:
    """Thread-safe analyzer that prevents recursion issues."""

    def __init__(self):
        self.analysis_lock = threading.Lock()
        self.active_analyses = set()
        self.thread_depth = threading.local()

    def analyze_with_concurrency_protection(self, target, analysis_id=None):
        """Analyze with both recursion and concurrency protection."""
        if not hasattr(self.thread_depth, 'depth'):
            self.thread_depth.depth = 0

        if analysis_id is None:
            analysis_id = f"analysis_{threading.current_thread().ident}_{time.time()}"

        # Thread-local recursion check
        if self.thread_depth.depth > 5:
            return {
                "error": "Thread-local recursion depth exceeded",
                "thread_id": threading.current_thread().ident,
                "depth": self.thread_depth.depth
            }

        # Global analysis tracking
        with self.analysis_lock:
            if analysis_id in self.active_analyses:
                return {
                    "error": "Analysis already in progress",
                    "analysis_id": analysis_id,
                    "active_count": len(self.active_analyses)
                }
            self.active_analyses.add(analysis_id)

        try:
            self.thread_depth.depth += 1

            # Simulate analysis work
            time.sleep(0.01)  # Brief work simulation

            if "recursive" in str(target).lower():
                # This might trigger recursive analysis
                sub_result = self.analyze_with_concurrency_protection(
                    "sub_" + str(target)[:50],
                    f"sub_{analysis_id}"
                )
                return {
                    "type": "recursive_analysis",
                    "sub_result": sub_result,
                    "thread_id": threading.current_thread().ident,
                    "depth": self.thread_depth.depth
                }
            else:
                return {
                    "type": "normal_analysis",
                    "target_length": len(str(target)),
                    "thread_id": threading.current_thread().ident,
                    "depth": self.thread_depth.depth
                }

        finally:
            self.thread_depth.depth -= 1
            with self.analysis_lock:
                self.active_analyses.discard(analysis_id)

    def concurrent_analysis_test(self, num_threads=5):
        """Test concurrent analysis scenarios."""
        results = queue.Queue()

        def worker(worker_id):
            """Worker function for concurrent testing."""
            try:
                # Each worker tries recursive analysis
                result = self.analyze_with_concurrency_protection(
                    f"recursive_target_worker_{worker_id}",
                    f"worker_{worker_id}_analysis"
                )
                results.put({"worker_id": worker_id, "result": result, "status": "success"})
            except Exception as e:
                results.put({"worker_id": worker_id, "error": str(e), "status": "error"})

        # Start multiple threads
        threads = []
        for i in range(num_threads):
            thread = threading.Thread(target=worker, args=(i,))
            threads.append(thread)
            thread.start()

        # Wait for completion
        for thread in threads:
            thread.join(timeout=5)

        # Collect results
        worker_results = []
        while not results.empty():
            worker_results.append(results.get())

        return {
            "concurrent_test_results": worker_results,
            "threads_completed": len(worker_results),
            "threads_started": num_threads,
            "all_threads_completed": len(worker_results) == num_threads
        }
''')

            # Test the concurrent analyzer
            audit_tool = AnalyzeCodebase(target_path=temp_dir)

            start_time = time.time()
            audit_result = audit_tool.run()
            end_time = time.time()

            # Should complete without deadlocks or infinite recursion
            assert (end_time - start_time) < 15, "Should handle concurrent recursion safely"

            audit_data = json.loads(audit_result)
            assert "qt_score" in audit_data

            # Should detect the concurrent protection methods
            assert audit_data["codebase_analysis"]["total_behaviors"] >= 2


class TestPhilosophicalCompletenessScenarios:
    """Test philosophical aspects of complete self-assessment systems."""

    def test_godel_incompleteness_analogy(self):
        """Test scenarios analogous to Gödel's incompleteness theorems in quality assessment."""
        with tempfile.TemporaryDirectory() as temp_dir:
            incomplete_assessor = Path(temp_dir) / "incomplete_assessor.py"
            incomplete_assessor.write_text('''
class IncompleteQualityAssessor:
    """Quality assessor that demonstrates incompleteness analogies."""

    def __init__(self):
        self.assessment_rules = []
        self.unprovable_statements = []

    def add_assessment_rule(self, rule_name, rule_function):
        """Add a rule for quality assessment."""
        self.assessment_rules.append({
            "name": rule_name,
            "function": rule_function
        })

    def assess_quality(self, code):
        """Assess quality using available rules."""
        results = {}

        for rule in self.assessment_rules:
            try:
                result = rule["function"](code)
                results[rule["name"]] = result
            except Exception as e:
                results[rule["name"]] = {"error": str(e)}

        return results

    def assess_self_consistency(self):
        """Attempt to assess own consistency - the incompleteness analogy."""
        # This is analogous to asking: "Can this system prove its own consistency?"

        # Define some rules
        def rule_functions_should_exist(code):
            return {"valid": "def " in code, "reason": "Functions should be defined"}

        def rule_no_infinite_loops(code):
            # This is actually undecidable in general!
            return {"valid": "while True:" not in code, "reason": "Avoid infinite loops"}

        def rule_self_reference_is_problematic(code):
            return {"valid": "assess_self" not in code, "reason": "Self-reference creates problems"}

        self.add_assessment_rule("functions_exist", rule_functions_should_exist)
        self.add_assessment_rule("no_infinite_loops", rule_no_infinite_loops)
        self.add_assessment_rule("no_self_reference", rule_self_reference_is_problematic)

        # Now assess our own code
        import inspect
        own_code = inspect.getsource(IncompleteQualityAssessor)

        self_assessment = self.assess_quality(own_code)

        # The paradox: if we have a rule against self-reference,
        # but we're doing self-assessment, we violate our own rule!
        consistency_check = {
            "self_assessment": self_assessment,
            "consistency_paradox": (
                "no_self_reference" in self_assessment and
                not self_assessment["no_self_reference"].get("valid", True)
            ),
            "incompleteness_demonstrated": True
        }

        return consistency_check

    def identify_unprovable_statements(self):
        """Identify statements about code quality that cannot be proven."""
        unprovable_statements = [
            {
                "statement": "This program will terminate",
                "reason": "Halting problem - undecidable in general",
                "impact_on_quality": "Cannot guarantee termination in quality assessment"
            },
            {
                "statement": "This assessment system is consistent",
                "reason": "Analogous to Gödel's second incompleteness theorem",
                "impact_on_quality": "Cannot prove our own assessment system is consistent"
            },
            {
                "statement": "This is the optimal quality score",
                "reason": "Optimality depends on unmeasurable factors",
                "impact_on_quality": "Quality assessment is inherently incomplete"
            },
            {
                "statement": "This code contains no bugs",
                "reason": "Bug detection is undecidable for arbitrary properties",
                "impact_on_quality": "Perfect quality assessment is impossible"
            }
        ]

        self.unprovable_statements = unprovable_statements
        return unprovable_statements

    def demonstrate_incompleteness_limitations(self):
        """Demonstrate fundamental limitations in quality assessment."""
        limitations = {
            "consistency_check": self.assess_self_consistency(),
            "unprovable_statements": self.identify_unprovable_statements(),
            "fundamental_limitation": (
                "Any sufficiently powerful quality assessment system "
                "cannot prove its own correctness or completeness"
            )
        }

        return limitations
''')

            # Audit this philosophical assessor
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            # Should complete despite the philosophical paradoxes
            assert "qt_score" in audit_data

            # Should detect the philosophical methods
            assert audit_data["codebase_analysis"]["total_behaviors"] >= 4

            # The very fact that we can audit this demonstrates both
            # the power and limitations of automated quality assessment

    def test_observer_effect_in_quality_measurement(self):
        """Test the observer effect - how measurement affects the measured system."""
        with tempfile.TemporaryDirectory() as temp_dir:
            observer_effect_demo = Path(temp_dir) / "observer_effect.py"
            observer_effect_demo.write_text('''
class ObserverEffectDemo:
    """Demonstrates how quality measurement affects the measured system."""

    def __init__(self):
        self.measurement_history = []
        self.state_changes_due_to_measurement = []

    def measure_quality(self, target_system):
        """Measure quality of target system."""
        measurement = {
            "timestamp": len(self.measurement_history),
            "system_state": self._capture_system_state(target_system),
            "quality_metrics": self._calculate_quality_metrics(target_system)
        }

        self.measurement_history.append(measurement)

        # The act of measurement changes the system!
        self._record_measurement_effect(target_system)

        return measurement

    def _capture_system_state(self, system):
        """Capture current state of system."""
        if hasattr(system, '__dict__'):
            return len(system.__dict__)
        elif isinstance(system, str):
            return len(system)
        else:
            return hash(str(system)) % 1000

    def _calculate_quality_metrics(self, system):
        """Calculate quality metrics."""
        return {
            "complexity": self._capture_system_state(system),
            "measurement_count": len(self.measurement_history)
        }

    def _record_measurement_effect(self, system):
        """Record how measurement affected the system."""
        # In real systems, the act of measurement (adding logging,
        # instrumentation, etc.) changes the system behavior

        effect = {
            "measurement_number": len(self.measurement_history),
            "system_modified": True,
            "modification_type": "instrumentation_added",
            "performance_impact": "minimal"  # In real systems, this could be significant
        }

        self.state_changes_due_to_measurement.append(effect)

        # Simulate the effect: measuring changes the system
        if hasattr(system, 'times_measured'):
            system.times_measured += 1
        else:
            # We're adding state to the system by measuring it!
            if hasattr(system, '__dict__'):
                system.times_measured = 1

    def demonstrate_observer_effect(self):
        """Demonstrate the observer effect in quality measurement."""

        class TestSystem:
            def __init__(self):
                self.original_state = "pristine"

        # Create a test system
        test_system = TestSystem()

        # Capture initial state
        initial_state = self._capture_system_state(test_system)

        # Perform multiple measurements
        measurements = []
        for i in range(5):
            measurement = self.measure_quality(test_system)
            measurements.append(measurement)

        # Capture final state
        final_state = self._capture_system_state(test_system)

        # Analyze the observer effect
        analysis = {
            "initial_state": initial_state,
            "final_state": final_state,
            "state_changed": initial_state != final_state,
            "measurements_performed": len(measurements),
            "system_modifications": self.state_changes_due_to_measurement,
            "observer_effect_demonstrated": (
                len(self.state_changes_due_to_measurement) > 0 and
                initial_state != final_state
            )
        }

        return analysis

    def measure_measurement_quality(self):
        """Measure the quality of our measurement process - meta-measurement."""
        # This creates another layer of observer effect!

        meta_measurement = {
            "measurement_process_quality": {
                "measurements_performed": len(self.measurement_history),
                "side_effects_tracked": len(self.state_changes_due_to_measurement),
                "observer_effect_awareness": True
            },
            "meta_observer_effect": {
                "measuring_measurement_changes_measurement": True,
                "recursion_depth": 2,
                "philosophical_implications": "Infinite regress of measurement"
            }
        }

        # The act of meta-measurement also has effects!
        self.state_changes_due_to_measurement.append({
            "type": "meta_measurement",
            "effect": "Added meta-measurement instrumentation"
        })

        return meta_measurement
''')

            # Audit the observer effect demonstration
            audit_tool = AnalyzeCodebase(target_path=temp_dir)
            audit_result = audit_tool.run()
            audit_data = json.loads(audit_result)

            # The very act of auditing this code demonstrates the observer effect!
            assert "qt_score" in audit_data

            # Should detect the measurement methods
            assert audit_data["codebase_analysis"]["total_behaviors"] >= 4

            # Generate tests that are aware of the observer effect
            audit_report = {
                "violations": [
                    {"property": "Y", "severity": "high", "description": "Observer effect testing needed"}
                ]
            }

            generate_tool = GenerateTests(
                audit_report=json.dumps(audit_report),
                target_file=str(observer_effect_demo)
            )

            generation_result = generate_tool.run()
            generation_data = json.loads(generation_result)

            assert generation_data["status"] == "success"

            # The generated tests themselves are part of the observer effect!
            test_file = generation_data["test_file"]
            with open(test_file, 'r') as f:
                test_content = f.read()

            # The tests should test the observer effect
            assert "observer" in test_content.lower() or "measurement" in test_content.lower()

            # Meta-philosophical note: By generating tests for the observer effect,
            # we are observing (and thus affecting) the system that observes systems!